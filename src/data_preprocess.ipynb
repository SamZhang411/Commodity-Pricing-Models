{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soybean Futures Contracts Pricing Model\n",
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WASDE Data- Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wasde_by_commodity(data):\n",
    "    \"\"\"\n",
    "    Filters data by commodity and region according to predefined criteria.\n",
    "    Returns a list of filtered DataFrames.\n",
    "    \"\"\"\n",
    "    filter_conditions = [\n",
    "        (\n",
    "            \"World Corn Supply and Use\",\n",
    "            [\"World\", \"Major exporters\", \"United States\", \"Major Exporters\"],\n",
    "        ),\n",
    "        (\n",
    "            \"World Wheat Supply and Use\",\n",
    "            [\"World\", \"Major exporters\", \"United States\", \"Major Exporters\"],\n",
    "        ),\n",
    "        (\n",
    "            \"World Soybean Supply and Use\",\n",
    "            [\"World\", \"Argentina\", \"Brazil\", \"United States\", \"Major Exporters\"],\n",
    "        ),\n",
    "        (\n",
    "            \"World Soybean Meal Supply and Use\",\n",
    "            [\"World\", \"Major exporters\", \"United States\", \"Major Exporters\"],\n",
    "        ),\n",
    "        (\n",
    "            \"World Soybean Oil Supply and Use\",\n",
    "            [\"World\", \"Major exporters\", \"United States\", \"Major Exporters\"],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    filtered_data = []\n",
    "    for title, regions in filter_conditions:\n",
    "        filtered = data[(data[\"ReportTitle\"] == title) & (data[\"Region\"].isin(regions))]\n",
    "        filtered_data.append(filtered)\n",
    "    return filtered_data\n",
    "\n",
    "def process_wasde_by_path(path, wasde_data):\n",
    "    \"\"\"\n",
    "    Processes and aggregate historical WASDE data by path.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path, low_memory=False)\n",
    "    data_collection = filter_wasde_by_commodity(data)\n",
    "\n",
    "    for data_set in data_collection:\n",
    "        data_set = data_set[data_set[\"ProjEstFlag\"] == \"Proj.\"]\n",
    "        grouped_data = data_set.groupby(\"ReleaseDate\")\n",
    "\n",
    "        for _, group in grouped_data:\n",
    "            filtered_data = group[\n",
    "                [\"ReleaseDate\", \"Commodity\", \"Region\", \"Attribute\", \"Value\", \"Unit\"]\n",
    "            ]\n",
    "            filtered_data = filtered_data.rename(columns={\"ReleaseDate\": \"Report Date\"})\n",
    "            wasde_data = pd.concat([wasde_data, filtered_data], ignore_index=True)\n",
    "\n",
    "    return wasde_data\n",
    "\n",
    "def clean_wasde_data(data):\n",
    "    \"\"\"\n",
    "    Cleans the data by coercing types and applying a mapping to the 'Attribute' column.\n",
    "    \"\"\"\n",
    "    conversion_dict = {\n",
    "        \"Domestic Feed\": \"Feed\",\n",
    "        \"Domestic Crush\": \"Crush\",\n",
    "        \"Domestic Total\": \"Total Use\",\n",
    "    }\n",
    "\n",
    "    data[\"Report Date\"] = pd.to_datetime(data[\"Report Date\"]).dt.date\n",
    "    data[\"Attribute\"] = data[\"Attribute\"].replace(conversion_dict)\n",
    "\n",
    "    data_collection = data.groupby([\"Report Date\", \"Commodity\"])\n",
    "    sorted_data = pd.concat([group for _, group in data_collection]).sort_values(\n",
    "        by=[\"Report Date\", \"Commodity\"]\n",
    "    )\n",
    "    \n",
    "    dtype_wasde_data = {\n",
    "        \"Report Date\": \"datetime64[ns]\",\n",
    "        \"Commodity\": \"category\",\n",
    "        \"Region\": \"category\",\n",
    "        \"Attribute\": \"category\",\n",
    "        \"Value\": \"float64\"\n",
    "    }\n",
    "    sorted_data = sorted_data.astype(dtype_wasde_data)\n",
    "\n",
    "    return sorted_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def process_wasde_data(excel_2000_2010, csv_2010_2020, csv_2021_2024, output_path):\n",
    "    \"\"\"\n",
    "    Aggregates WASDE data from multiple sources and saves it to a Parquet file.\n",
    "    \"\"\"\n",
    "    # Load and concatenate Excel data, 2000 - 2010\n",
    "    wasde_raw = pd.read_excel(excel_2000_2010, sheet_name=None)\n",
    "    wasde_data = pd.concat(wasde_raw.values(), ignore_index=True)\n",
    "    wasde_data.rename(columns={\"Country\": \"Region\"}, inplace=True)\n",
    "\n",
    "    # Process CSV paths, 2010 - 2020\n",
    "    # processed_data = pd.DataFrame()\n",
    "    for path in csv_2010_2020:\n",
    "        wasde_data = process_wasde_by_path(path, wasde_data)\n",
    "\n",
    "    # Process files in the directory, 2021 - 2024\n",
    "    csv_dir_path = Path(csv_2021_2024)\n",
    "    for file_path in csv_dir_path.glob(\"*.csv\"):\n",
    "        wasde_data = process_wasde_by_path(file_path, wasde_data)\n",
    "\n",
    "    # wasde_data = pd.concat([wasde_data, processed_data], ignore_index=True)\n",
    "    wasde_data = clean_wasde_data(wasde_data)\n",
    "    wasde_data.to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wasde_data(input_path, output_path):\n",
    "    processed_data = pd.read_parquet(input_path)\n",
    "    processed_data = processed_data.drop(columns=[\"Unit\"])\n",
    "    processed_rows = []\n",
    "    replacements = {\n",
    "        \"OilSeed, Soybeans\": \"Soybeans\",\n",
    "        \"Oilseed, Soybean\": \"Soybeans\",\n",
    "        \"Meal, Soybeans\": \"Soybean Meal\",\n",
    "        \"Oil, Soybeans\": \"Soybean Oil\",\n",
    "    }\n",
    "\n",
    "    # Group by 'Report Date', 'Commodity', and 'Region'\n",
    "    grouped = processed_data.groupby([\"Report Date\", \"Commodity\", \"Region\"])\n",
    "\n",
    "    for (report_date, commodity, region), group in grouped:\n",
    "        # Initialize a dictionary for the row\n",
    "        aggregate_row = {\n",
    "            \"Report Date\": report_date,\n",
    "            \"Commodity\": commodity,\n",
    "            \"Region\": region,\n",
    "            \"Beginning Stocks\": None,\n",
    "            \"Production\": None,\n",
    "            \"Imports\": None,\n",
    "            \"Exports\": None,\n",
    "            \"Feed/Crush\": None,\n",
    "            \"Total Use\": None,\n",
    "            \"Ending Stocks\": None,\n",
    "            \"STU\": None,\n",
    "        }\n",
    "\n",
    "        # Fill the row dictionary based on the 'Attribute' column\n",
    "        ending_stocks = None\n",
    "        total_use = None\n",
    "        stu = None\n",
    "        for _, record in group.iterrows():\n",
    "            attribute = record[\"Attribute\"]\n",
    "            value = record[\"Value\"]\n",
    "\n",
    "            if attribute == \"Beginning Stocks\":\n",
    "                aggregate_row[\"Beginning Stocks\"] = value\n",
    "            elif attribute == \"Production\":\n",
    "                aggregate_row[\"Production\"] = value\n",
    "            elif attribute == \"Imports\":\n",
    "                aggregate_row[\"Imports\"] = value\n",
    "            elif attribute == \"Exports\":\n",
    "                aggregate_row[\"Exports\"] = value\n",
    "            elif attribute in [\"Feed\", \"Crush\"]:\n",
    "                aggregate_row[\"Feed/Crush\"] = value\n",
    "            elif attribute in [\"Total Use\", \"Use, Total\"]:\n",
    "                aggregate_row[\"Total Use\"] = value\n",
    "                total_use = value\n",
    "            elif attribute == \"Ending Stocks\":\n",
    "                aggregate_row[\"Ending Stocks\"] = value\n",
    "                ending_stocks = value\n",
    "\n",
    "        if ending_stocks and total_use is not None:\n",
    "            stu = round(ending_stocks / total_use, 4)\n",
    "            aggregate_row[\"STU\"] = stu\n",
    "\n",
    "        # Append the row to the list of processed rows\n",
    "        processed_rows.append(aggregate_row)\n",
    "\n",
    "    processed_data = pd.DataFrame(processed_rows)\n",
    "    processed_data[\"Commodity\"] = processed_data[\"Commodity\"].replace(replacements)\n",
    "\n",
    "    dtype_aggregate_data = {\n",
    "        \"Report Date\": \"datetime64[ns]\",\n",
    "        \"Commodity\": \"category\",\n",
    "        \"Region\": \"category\",\n",
    "        \"Beginning Stocks\": \"float64\",\n",
    "        \"Production\": \"float64\",\n",
    "        \"Imports\": \"float64\",\n",
    "        \"Exports\": \"float64\",\n",
    "        \"Feed/Crush\": \"float64\",\n",
    "        \"Total Use\": \"float64\",\n",
    "        \"Ending Stocks\": \"float64\",\n",
    "        \"STU\": \"float64\"\n",
    "    }\n",
    "\n",
    "    processed_data = processed_data.astype(dtype_aggregate_data)\n",
    "    processed_data.to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wasde_soybeans(data_path, output_path):\n",
    "    data = pd.read_parquet(data_path)\n",
    "    data[\"Report Date\"] = pd.to_datetime(data[\"Report Date\"])\n",
    "    data[\"Report Month\"] = data[\"Report Date\"].dt.to_period(\"M\")\n",
    "    filtered_rows = []\n",
    "\n",
    "    grouped = data.groupby(\"Report Date\")\n",
    "\n",
    "    for report_date, group in grouped:\n",
    "        soybean_row = {\n",
    "            \"Report Date\": report_date,\n",
    "            \"Report Month\": report_date.to_period(\"M\"),\n",
    "            \"STU, US\": None,\n",
    "            \"STU, AR\": None,\n",
    "            \"STU, BR\": None,\n",
    "            \"STU, Corn\": None,\n",
    "            \"Production, US\": None,\n",
    "            \"Production, AR\": None,\n",
    "            \"Production, BR\": None,\n",
    "        }\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            if row[\"Commodity\"] == \"Soybeans\":\n",
    "                if row[\"Region\"] == \"United States\":\n",
    "                    soybean_row[\"STU, US\"] = row[\"STU\"]\n",
    "                    soybean_row[\"Production, US\"] = row[\"Production\"]\n",
    "                elif row[\"Region\"] == \"Argentina\":\n",
    "                    soybean_row[\"STU, AR\"] = row[\"STU\"]\n",
    "                    soybean_row[\"Production, AR\"] = row[\"Production\"]\n",
    "                elif row[\"Region\"] == \"Brazil\":\n",
    "                    soybean_row[\"STU, BR\"] = row[\"STU\"]\n",
    "                    soybean_row[\"Production, BR\"] = row[\"Production\"]\n",
    "\n",
    "            if row[\"Commodity\"] == \"Corn\" and row[\"Region\"] == \"United States\":\n",
    "                soybean_row[\"STU, Corn\"] = row[\"STU\"]\n",
    "\n",
    "        # Append the new row to the list\n",
    "        filtered_rows.append(soybean_row)\n",
    "\n",
    "    dtype_soybean_row = {\n",
    "        \"Report Date\": \"datetime64[ns]\",\n",
    "        \"Report Month\": \"datetime64[ns]\",\n",
    "        \"STU, US\": \"float64\",\n",
    "        \"STU, AR\": \"float64\",\n",
    "        \"STU, BR\": \"float64\",\n",
    "        \"STU, Corn\": \"float64\",\n",
    "        \"Production, US\": \"float64\",\n",
    "        \"Production, AR\": \"float64\",\n",
    "        \"Production, BR\": \"float64\",\n",
    "    }\n",
    "\n",
    "    processed_data = pd.DataFrame(filtered_rows)\n",
    "    processed_data = processed_data.astype(dtype_soybean_row)\n",
    "    processed_data.to_parquet(output_path, index=False)\n",
    "\n",
    "def append_indicators(data_path, indicator_path, output_path):\n",
    "    # Read the main data and indicators data\n",
    "    data = pd.read_parquet(data_path)\n",
    "    indicators = pd.read_csv(indicator_path,low_memory=False)\n",
    "\n",
    "    # Convert 'Report Month' in df_data and 'Date' in df_indicator to period format (YYYY-MM)\n",
    "    data[\"Report Month\"] = pd.to_datetime(data[\"Report Date\"]).dt.to_period(\"M\")\n",
    "    indicators[\"Date\"] = pd.to_datetime(indicators[\"Date\"]).dt.to_period(\"M\")\n",
    "\n",
    "    # Merge the main data with the indicators data on 'Report Month' and 'Date'\n",
    "    merged_data = pd.merge(\n",
    "        data, indicators, how=\"left\", left_on=\"Report Month\", right_on=\"Date\"\n",
    "    )\n",
    "    \n",
    "    dtype_merged_data = {\n",
    "        \"Report Date\": \"datetime64[D]\",\n",
    "        \"STU, US\": \"float64\",\n",
    "        \"STU, AR\": \"float64\",\n",
    "        \"STU, BR\": \"float64\",\n",
    "        \"STU, Corn\": \"float64\",\n",
    "        \"Production, US\": \"float64\",\n",
    "        \"Production, AR\": \"float64\",\n",
    "        \"Production, BR\": \"float64\",\n",
    "        \"GDP (Bn USD)\": \"float64\",\n",
    "        \"Gold\": \"float64\",\n",
    "        \"DX\": \"float64\",\n",
    "        \"Crude\": \"float64\",\n",
    "        \"USD?BRL\": \"float64\"\n",
    "    }\n",
    "\n",
    "    merged_data.drop(columns=[\"Report Month\", \"Date\"], inplace=True)\n",
    "    merged_data = merged_data.astype(dtype_merged_data)\n",
    "    merged_data.to_parquet(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WASDE- Excecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "wasde_2000_2010 = \"../data/raw/wasde/2000-2010/World_WASDE_2000-2010.xlsx\"\n",
    "wasde_2010_2020 = [\n",
    "    \"../data/raw/wasde/2010-2020/WASDE_2010-2015.csv\",\n",
    "    \"../data/raw/wasde/2010-2020/WASDE_2016-2020.csv\",\n",
    "]\n",
    "wasde_2021_2024 = \"../data/raw/wasde/2021-2024\"\n",
    "processed_data_path = \"../data/interim/wasde.parquet\"\n",
    "aggregated_data_path = \"../data/interim/wasde_aggregate.parquet\"\n",
    "filtered_soybean_data_path = \"../data/interim/wasde_soybeans.parquet\"\n",
    "indicators_path = \"../data/interim/macroeconomic_indicators.csv\"\n",
    "output_path = \"../data/processed/soybeans_model_training_data.parquet\"\n",
    "\n",
    "# Run the aggregation process\n",
    "process_wasde_data(\n",
    "    wasde_2000_2010, wasde_2010_2020, wasde_2021_2024, processed_data_path\n",
    ")\n",
    "aggregate_wasde_data(processed_data_path, aggregated_data_path)\n",
    "filter_wasde_soybeans(aggregated_data_path, filtered_soybean_data_path)\n",
    "append_indicators(filtered_soybean_data_path, indicators_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical Pricing Data- Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_contract_names(csv_files, symbols, years):\n",
    "    \"\"\"Sorts the CSV files by expiration based on symbols and years.\"\"\"\n",
    "    contracts_sorted_by_expiration = []\n",
    "    for year in years:\n",
    "        for symbol in symbols:\n",
    "            contract_name = f\"{symbol}{str(year)[-2:]}.csv\"\n",
    "            if contract_name in csv_files:\n",
    "                contracts_sorted_by_expiration.append(contract_name)\n",
    "    return contracts_sorted_by_expiration\n",
    "\n",
    "\n",
    "def generate_price_data(row):\n",
    "    price_high = row[\"High\"]\n",
    "    price_low = row[\"Low\"]\n",
    "    \n",
    "    if pd.isna(price_high) or pd.isna(price_low):\n",
    "        return json.dumps({})\n",
    "    else:\n",
    "        return json.dumps({\"high\": float(price_high), \"low\": float(price_low)})\n",
    "\n",
    "\n",
    "def process_raw_price_data(csv_dir, contracts_sorted_by_expiration, all_price_data):\n",
    "    \"\"\"Merges contract data into the main DataFrame.\"\"\"\n",
    "    all_price_data['Date'] = pd.to_datetime(all_price_data['Date'])\n",
    "    for contract in contracts_sorted_by_expiration:\n",
    "        contract_name = contract.replace(\".csv\", \"\")\n",
    "        path = os.path.join(csv_dir, contract)\n",
    "\n",
    "        # Read and process the contract CSV\n",
    "        contract_price_data = pd.read_csv(path).iloc[:-1]\n",
    "        contract_price_data = contract_price_data.rename(columns={\"Time\": \"Date\"})\n",
    "        contract_price_data[\"Date\"] = pd.to_datetime(contract_price_data[\"Date\"])\n",
    "        contract_price_data[\"Price\"] = contract_price_data.apply(generate_price_data, axis=1)\n",
    "\n",
    "        # Merge into the main DataFrame\n",
    "        all_price_data = all_price_data.merge(\n",
    "            contract_price_data[[\"Date\", \"Price\"]],\n",
    "            on=\"Date\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", f\"_{contract_name}\"),\n",
    "        )\n",
    "        all_price_data.rename(columns={\"Price\": f\"{contract_name}\"}, inplace=True)\n",
    "\n",
    "    return all_price_data.fillna(\"\")\n",
    "\n",
    "\n",
    "def aggregate_price_data(data_dir, output_dir):\n",
    "    dates_path = os.path.join(output_dir, \"trading_dates.parquet\")\n",
    "    aggregate_price_path = os.path.join(output_dir, \"prices_soybeans_aggregate.parquet\")\n",
    "\n",
    "    trading_dates = pd.read_parquet(dates_path)\n",
    "\n",
    "    data = pd.DataFrame(trading_dates, columns=[\"Date\"])\n",
    "    symbols = [\"SF\", \"SH\", \"SK\", \"SN\", \"SQ\", \"SU\", \"SX\"]\n",
    "    years = list(range(2000, pd.Timestamp.today().year + 2))\n",
    "\n",
    "    contract_csv_collection = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "    contracts_sorted = get_sorted_contract_names(contract_csv_collection, symbols, years)\n",
    "\n",
    "    data = process_raw_price_data(data_dir, contracts_sorted, data)\n",
    "    data.to_parquet(aggregate_price_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_prices(price_json):\n",
    "    if not price_json or price_json == '{}':\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        price_dict = json.loads(price_json)\n",
    "        high = price_dict.get(\"high\", None)\n",
    "        low = price_dict.get(\"low\", None)\n",
    "        return float(high), float(low) if high is not None and low is not None else (None, None)\n",
    "    except (json.JSONDecodeError, ValueError, TypeError):\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def determine_contract(month, year, date, wasde_date):\n",
    "    \"\"\"Determines the correct contract based on the month, year, and WASDE date.\"\"\"\n",
    "    yr = str(year)[-2:]  # Get the last two digits of the year\n",
    "    contracts = {\n",
    "        1: (f\"SF{yr}\", f\"SH{yr}\"),\n",
    "        2: (f\"SH{yr}\", f\"SH{yr}\"),\n",
    "        3: (f\"SH{yr}\", f\"SK{yr}\"),\n",
    "        4: (f\"SK{yr}\", f\"SK{yr}\"),\n",
    "        5: (f\"SK{yr}\", f\"SN{yr}\"),\n",
    "        6: (f\"SN{yr}\", f\"SN{yr}\"),\n",
    "        7: (f\"SN{yr}\", f\"SQ{yr}\"),\n",
    "        8: (f\"SQ{yr}\", f\"SU{yr}\"),\n",
    "        9: (f\"SU{yr}\", f\"SX{yr}\"),\n",
    "        10: (f\"SX{yr}\", f\"SX{yr}\"),\n",
    "        11: (f\"SX{yr}\", f\"SF{str(int(year) + 1)[-2:]}\"),\n",
    "        12: (f\"SF{str(int(year) + 1)[-2:]}\", f\"SF{str(int(year) + 1)[-2:]}\"),\n",
    "    }\n",
    "    \n",
    "    current_contract, next_contract = contracts.get(month, (\"\", \"\"))\n",
    "    \n",
    "    # Decide whether to stay on the current contract or move to the next\n",
    "    if date >= wasde_date:\n",
    "        return next_contract\n",
    "    else:\n",
    "        return current_contract\n",
    "\n",
    "def process_continuous_data(trading_dates, wasde_dates, price_data):\n",
    "    \"\"\"Processes continuous futures data by selecting the appropriate contracts.\"\"\"\n",
    "    continuous_data = []\n",
    "    wasde_iter = iter(wasde_dates[\"Report Date\"])\n",
    "    current_wasde_date = next(wasde_iter, None)\n",
    "    contract_name = None\n",
    "\n",
    "    for date in trading_dates[\"Date\"]:\n",
    "        if current_wasde_date is not None and date >= current_wasde_date:\n",
    "            # Determine contract before rolling over to the next WASDE date\n",
    "            month, year = date.month, date.year\n",
    "            contract_name = determine_contract(month, year, date, current_wasde_date)\n",
    "            \n",
    "            # Roll over to the next WASDE date\n",
    "            try:\n",
    "                current_wasde_date = next(wasde_iter)\n",
    "            except StopIteration:\n",
    "                current_wasde_date = wasde_dates[\"Report Date\"].iloc[-1]\n",
    "        \n",
    "        # Use the determined contract name until the date surpasses the current_wasde_date\n",
    "        if (\n",
    "            contract_name\n",
    "            and not price_data.loc[\n",
    "                price_data[\"Date\"] == date, contract_name\n",
    "            ].empty\n",
    "        ):\n",
    "            daily_price_json = price_data.loc[\n",
    "                price_data[\"Date\"] == date, contract_name\n",
    "            ].values[0]\n",
    "            high, low = unpack_prices(daily_price_json)\n",
    "\n",
    "            if high is not None and low is not None:\n",
    "                average = (high + low) / 2\n",
    "                continuous_data.append([date, contract_name, high, low, average])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        continuous_data, columns=[\"Date\", \"Contract\", \"High\", \"Low\", \"Average\"]\n",
    "    )    \n",
    "\n",
    "def get_next_contracts(base_contract):\n",
    "    \"\"\"Gets the base contract and the next 6 contracts in the sequence following the base contract.\"\"\"\n",
    "    contract_sequence = [\"SF\", \"SH\", \"SK\", \"SN\", \"SQ\", \"SU\", \"SX\"]\n",
    "\n",
    "    # Ensure year suffix is two digits\n",
    "    year_suffix = base_contract[-2:]\n",
    "    base_contract_name = base_contract[:-2]\n",
    "    base_index = contract_sequence.index(base_contract_name)\n",
    "\n",
    "    # Start with contracts from the current year\n",
    "    contracts = [\n",
    "        f\"{contract}{year_suffix}\" for contract in contract_sequence[base_index:]\n",
    "    ]\n",
    "\n",
    "    # If less than 7 contracts, include contracts from the next year\n",
    "    if len(contracts) < 7:\n",
    "        next_year_suffix = f\"{(int(year_suffix) + 1) % 100:02}\"  # Ensure two-digit format\n",
    "        contracts.extend(\n",
    "            [\n",
    "                f\"{contract}{next_year_suffix}\"\n",
    "                for contract in contract_sequence[: 7 - len(contracts)]\n",
    "            ]\n",
    "        )\n",
    "    return contracts[:7]  # Ensure the list is exactly 7 contracts long\n",
    "\n",
    "def process_year_ahead_pricing_data(trading_dates, wasde_dates, price_data):\n",
    "    \"\"\"Processes continuous futures data by selecting the appropriate contracts.\"\"\"\n",
    "    continuous_data_ext = []\n",
    "    wasde_iter = iter(wasde_dates[\"Report Date\"])\n",
    "    current_wasde_date = next(wasde_iter, None)\n",
    "    contract_name = None\n",
    "    contract_collection = []\n",
    "\n",
    "    for date in trading_dates[\"Date\"]:\n",
    "        if current_wasde_date is not None and date >= current_wasde_date:\n",
    "            month, year = date.month, date.year\n",
    "            contract_name = determine_contract(month, year, date, current_wasde_date)\n",
    "            contract_collection = get_next_contracts(contract_name)\n",
    "\n",
    "            try:\n",
    "                current_wasde_date = next(wasde_iter)\n",
    "            except StopIteration:\n",
    "                current_wasde_date = wasde_dates[\"Report Date\"].iloc[-1]\n",
    "\n",
    "        for contract in contract_collection:\n",
    "            if (\n",
    "            contract\n",
    "            and not price_data.loc[\n",
    "                price_data[\"Date\"] == date, contract\n",
    "            ].empty\n",
    "            ):\n",
    "                daily_price_json = price_data.loc[\n",
    "                    price_data[\"Date\"] == date, contract\n",
    "                ].values[0]\n",
    "                high, low = unpack_prices(daily_price_json)\n",
    "\n",
    "                if high is not None and low is not None:\n",
    "                    average = (high + low) / 2\n",
    "                    continuous_data_ext.append([date, contract, high, low, average])\n",
    "    return pd.DataFrame(\n",
    "        continuous_data_ext, columns=[\"Date\", \"Contract\", \"High\", \"Low\", \"Average\"]\n",
    "    )\n",
    "\n",
    "def generate_continuous_price_data(data_path, output_dir):\n",
    "    dates_path = os.path.join(output_dir, \"trading_dates.parquet\")\n",
    "    aggregate_price_path = os.path.join(output_dir, \"prices_soybeans_aggregate.parquet\")\n",
    "    continuous_price_path = os.path.join(output_dir, \"prices_soybeans_continuous.parquet\")\n",
    "    continuous_price_ext_path = os.path.join(output_dir, \"prices_soybeans_continuous_ext.parquet\")\n",
    "\n",
    "    price_data = pd.read_parquet(aggregate_price_path)\n",
    "    trading_dates = pd.read_parquet(dates_path)\n",
    "    wasde_dates = pd.read_parquet(data_path)\n",
    "\n",
    "    trading_dates[\"Date\"] = pd.to_datetime(trading_dates[\"Date\"])\n",
    "    wasde_dates[\"Report Date\"] = pd.to_datetime(wasde_dates[\"Report Date\"])\n",
    "\n",
    "    price_continuous = process_continuous_data(trading_dates, wasde_dates, price_data)\n",
    "    price_continuous.to_parquet(continuous_price_path, index=False)\n",
    "    price_ext = process_year_ahead_pricing_data(trading_dates, wasde_dates, price_data)\n",
    "    price_ext.to_parquet(continuous_price_ext_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pricing Data- Execution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/raw/historical_prices/soybeans\"\n",
    "output_dir = \"../data/interim/\"\n",
    "wasde_path = \"../data/interim/wasde_soybeans.parquet\"\n",
    "\n",
    "aggregate_price_data(data_dir, output_dir)\n",
    "generate_continuous_price_data(wasde_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate All Model Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_path = \"../data/interim/prices_soybeans_continuous.parquet\"\n",
    "data_path = \"../data/processed/soybeans_model_training_data.parquet\"\n",
    "output_csv_path = \"../data/processed/soybeans_model_training_data.csv\"\n",
    "\n",
    "processed_data = pd.read_parquet(data_path)\n",
    "daily_price_data = pd.read_parquet(price_path)\n",
    "\n",
    "# Standardize date columns to datetime.date\n",
    "processed_data.rename(columns={\"Report Date\": \"Date\"}, inplace=True)\n",
    "processed_data[\"Date\"] = pd.to_datetime(processed_data[\"Date\"]).dt.date\n",
    "daily_price_data[\"Date\"] = pd.to_datetime(daily_price_data[\"Date\"]).dt.date\n",
    "\n",
    "# Initialize lists to store results\n",
    "price_high_list = []\n",
    "price_low_list = []\n",
    "price_average_list = []\n",
    "price_collections = []\n",
    "\n",
    "# Create an iterator over the report dates\n",
    "report_date_iter = iter(processed_data[\"Date\"])\n",
    "\n",
    "# Get the first report date\n",
    "start_date = next(report_date_iter, None)\n",
    "\n",
    "# Iterate through report dates\n",
    "for end_date in report_date_iter:\n",
    "    # Filter the daily_price_data for the date range\n",
    "    group = daily_price_data[(daily_price_data[\"Date\"] >= start_date) & (daily_price_data[\"Date\"] < end_date)]\n",
    "\n",
    "    if not group.empty:\n",
    "        group = group.head(15)  # Limit to the first 15 rows\n",
    "        price_array = group[\"Average\"].tolist()\n",
    "\n",
    "        price_high = group[\"High\"].max()\n",
    "        price_low = group[\"Low\"].min()\n",
    "        price_average = group[\"Average\"].mean()\n",
    "    else:\n",
    "        # If the group is empty, assign NaN or an appropriate default value\n",
    "        price_array = []\n",
    "        price_high = float(\"nan\")\n",
    "        price_low = float(\"nan\")\n",
    "        price_average = float(\"nan\")\n",
    "\n",
    "    # Append the results to the respective lists\n",
    "    price_high_list.append(float(price_high))\n",
    "    price_low_list.append(float(price_low))\n",
    "    price_average_list.append(float(price_average))\n",
    "    price_collections.append(price_array)\n",
    "\n",
    "    # Update the start date to the current end date for the next iteration\n",
    "    start_date = end_date\n",
    "\n",
    "# Handle the last report date (no next end date)\n",
    "group = daily_price_data[daily_price_data[\"Date\"] >= start_date]\n",
    "\n",
    "if not group.empty:\n",
    "    group = group.head(15)\n",
    "    price_array = group[\"Average\"].tolist()\n",
    "\n",
    "    price_high = group[\"High\"].max()\n",
    "    price_low = group[\"Low\"].min()\n",
    "    price_average = group[\"Average\"].mean()\n",
    "else:\n",
    "    price_array = []\n",
    "    price_high = float(\"nan\")\n",
    "    price_low = float(\"nan\")\n",
    "    price_average = float(\"nan\")\n",
    "\n",
    "# Append the results to the lists for the last report date\n",
    "price_high_list.append(float(price_high))\n",
    "price_low_list.append(float(price_low))\n",
    "price_average_list.append(float(price_average))\n",
    "price_collections.append(price_array)\n",
    "\n",
    "# Add the new columns to the DataFrame\n",
    "processed_data[\"Price_High\"] = price_high_list\n",
    "processed_data[\"Price_Low\"] = price_low_list\n",
    "processed_data[\"Price_Average\"] = price_average_list\n",
    "processed_data[\"Average_Price_Collection\"] = price_collections\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "processed_data.to_parquet(data_path)\n",
    "processed_data.to_csv(output_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
