{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_commodity(data):\n",
    "    \"\"\"\n",
    "    Filters data by commodity and region according to predefined criteria.\n",
    "    Returns a list of filtered DataFrames.\n",
    "    \"\"\"\n",
    "    filter_conditions = [\n",
    "        ('World Corn Supply and Use', ['World', 'Major exporters', 'United States', 'Major Exporters']),\n",
    "        ('World Wheat Supply and Use', ['World', 'Major exporters', 'United States', 'Major Exporters']),\n",
    "        ('World Soybean Supply and Use', ['World', 'Argentina', 'Brazil', 'United States', 'Major Exporters']),\n",
    "        ('World Soybean Meal Supply and Use', ['World', 'Major exporters', 'United States', 'Major Exporters']),\n",
    "        ('World Soybean Oil Supply and Use', ['World', 'Major exporters', 'United States', 'Major Exporters'])\n",
    "    ]\n",
    "    \n",
    "    filtered_data = []\n",
    "    for title, regions in filter_conditions:\n",
    "        filtered = data[(data['ReportTitle'] == title) &\n",
    "                        (data['Region'].isin(regions))]\n",
    "        filtered_data.append(filtered)\n",
    "    return filtered_data\n",
    "\n",
    "def process_by_release_date(data_collection, wasde_data):\n",
    "    \"\"\"\n",
    "    Processes each DataFrame in data_collection, filtering by release date and\n",
    "    concatenating to wasde_data.\n",
    "    \"\"\"\n",
    "    for data_set in data_collection:\n",
    "        data_set = data_set[data_set['ProjEstFlag'] == 'Proj.']\n",
    "        grouped_data = data_set.groupby('ReleaseDate')\n",
    "        \n",
    "        for release_date, group in grouped_data:\n",
    "            filtered_data = group[[\"ReleaseDate\", \"Commodity\", \"Region\", \"Attribute\", \"Value\", \"Unit\"]]\n",
    "            filtered_data = filtered_data.rename(columns={\"ReleaseDate\": \"Report Date\"})\n",
    "            wasde_data = pd.concat([wasde_data, filtered_data], ignore_index=True)\n",
    "    \n",
    "    return wasde_data\n",
    "\n",
    "def process_by_path(path, wasde_data):\n",
    "    \"\"\"\n",
    "    Processes data from a given path and appends it to wasde_data.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path, low_memory=False)\n",
    "    filtered_data = filter_by_commodity(data)\n",
    "    processed_data = process_by_release_date(filtered_data, wasde_data)\n",
    "    # processed_data = processed_data.sort_values(by='Report Date', ascending=True)\n",
    "    return processed_data\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Cleans the data by coercing types and applying a mapping to the 'Attribute' column.\n",
    "    \"\"\"\n",
    "    conversion_dict = {\n",
    "        'Domestic Feed': 'Feed',\n",
    "        'Domestic Crush': 'Crush',\n",
    "        'Domestic Total': 'Total Use',\n",
    "    }\n",
    "\n",
    "    data['Report Date'] = pd.to_datetime(data['Report Date']).dt.date\n",
    "    data['Commodity'] = data['Commodity'].astype(str)\n",
    "    data['Region'] = data['Region'].astype(str)\n",
    "    data['Attribute'] = data['Attribute'].astype(str)\n",
    "    data['Value'] = data['Value'].astype(float)\n",
    "    data['Unit'] = data['Unit'].astype(str)\n",
    "    data['Attribute'] = data['Attribute'].replace(conversion_dict)\n",
    "\n",
    "    data_collection = data.groupby(['Report Date', 'Commodity'])\n",
    "    sorted_data = pd.concat([group for _, group in data_collection]).sort_values(by=['Report Date', 'Commodity'])\n",
    "\n",
    "    return sorted_data.reset_index(drop=True)\n",
    "\n",
    "def process_wasde_data(excel_path, csv_paths, csv_dir, output_path):\n",
    "    \"\"\"\n",
    "    Aggregates WASDE data from multiple sources and saves it to a CSV file.\n",
    "    \"\"\"\n",
    "    # Load and concatenate Excel data\n",
    "    WASDE_df_dict = pd.read_excel(excel_path, sheet_name=None)\n",
    "    WASDE_df = pd.concat(WASDE_df_dict.values(), ignore_index=True)\n",
    "    WASDE_df.rename(columns={\"Country\": \"Region\"}, inplace=True)\n",
    "\n",
    "    # Process CSV paths\n",
    "    processed_data = pd.DataFrame()\n",
    "    for path in csv_paths:\n",
    "        processed_data = process_by_path(path, processed_data)\n",
    "\n",
    "    # Process files in the CSV directory\n",
    "    csv_dir_path = Path(csv_dir)\n",
    "    for file_path in csv_dir_path.glob('*.csv'):\n",
    "        processed_data = process_by_path(file_path, processed_data)\n",
    "    \n",
    "    # Combine all data\n",
    "    WASDE_df = pd.concat([WASDE_df, processed_data], ignore_index=True)\n",
    "\n",
    "    # Clean the data and save to CSV\n",
    "    WASDE_df = clean_data(WASDE_df)\n",
    "    WASDE_df.to_csv(output_path, index=False)\n",
    "\n",
    "def aggregate_wasde_data(input_path, output_path):\n",
    "    # Drop the 'Unit' column\n",
    "    WASDE_df = pd.read_csv(input_path, low_memory=False)\n",
    "    WASDE_df = WASDE_df.drop(columns=['Unit'])\n",
    "    \n",
    "    # Initialize a list to store the processed rows\n",
    "    processed_rows = []\n",
    "    \n",
    "    # Group by 'Report Date', 'Commodity', and 'Region'\n",
    "    grouped = WASDE_df.groupby(['Report Date', 'Commodity', 'Region'])\n",
    "    \n",
    "    for (report_date, commodity, region), group in grouped:\n",
    "        # Initialize a dictionary for the row\n",
    "        row = {\n",
    "            'Report Date': report_date,\n",
    "            'Commodity': commodity,\n",
    "            'Region': region,\n",
    "            'Beginning Stocks': None,\n",
    "            'Production': None,\n",
    "            'Imports': None,\n",
    "            'Exports': None,\n",
    "            'Feed/Crush': None,\n",
    "            'Total Use': None,\n",
    "            'Ending Stocks': None,\n",
    "            'STU': None\n",
    "        }\n",
    "        \n",
    "        # Fill the row dictionary based on the 'Attribute' column\n",
    "        ending_stocks = None\n",
    "        total_use = None\n",
    "        stu = None\n",
    "        for _, record in group.iterrows():\n",
    "            attribute = record['Attribute']\n",
    "            value = record['Value']\n",
    "            \n",
    "            if attribute == 'Beginning Stocks':\n",
    "                row['Beginning Stocks'] = value\n",
    "            elif attribute == 'Production':\n",
    "                row['Production'] = value\n",
    "            elif attribute == 'Imports':\n",
    "                row['Imports'] = value\n",
    "            elif attribute == 'Exports':\n",
    "                row['Exports'] = value\n",
    "            elif attribute in ['Feed', 'Crush']:\n",
    "                row['Feed/Crush'] = value\n",
    "            elif attribute in ['Total Use', 'Use, Total']:\n",
    "                row['Total Use'] = value\n",
    "                total_use = value\n",
    "            elif attribute == 'Ending Stocks':\n",
    "                row['Ending Stocks'] = value\n",
    "                ending_stocks = value\n",
    "\n",
    "        if ending_stocks and total_use is not None:\n",
    "            stu = round(ending_stocks / total_use, 4)\n",
    "            row['STU'] = stu \n",
    "\n",
    "        # Append the row to the list of processed rows\n",
    "        processed_rows.append(row)\n",
    "    \n",
    "    # Create a new DataFrame from the processed rows\n",
    "    processed_df = pd.DataFrame(processed_rows)\n",
    "    replacements = {\n",
    "    'OilSeed, Soybeans': 'Soybeans',\n",
    "    'Oilseed, Soybean': 'Soybeans',\n",
    "    'Meal, Soybeans': 'Soybean Meal',\n",
    "    'Oil, Soybeans': 'Soybean Oil'\n",
    "    }\n",
    "\n",
    "    # Replace the entries in the 'Commodity' column\n",
    "    processed_df['Commodity'] = processed_df['Commodity'].replace(replacements)\n",
    "\n",
    "    # Save the new DataFrame to a CSV file\n",
    "    processed_df.to_csv(output_path, index=False)\n",
    "\n",
    "def process_wasde_soybeans(data_path, output_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['Report Date'] = pd.to_datetime(data['Report Date'])\n",
    "    data['Report Month'] = data['Report Date'].dt.to_period('M')\n",
    "    rows = []\n",
    "\n",
    "    grouped = data.groupby('Report Date')\n",
    "\n",
    "    for report_date, group in grouped:\n",
    "        new_row = {\n",
    "            'Report Date': report_date,\n",
    "            'Report Month': report_date.to_period('M'),\n",
    "            'STU, US': None,\n",
    "            'STU, AR': None,\n",
    "            'STU, BR': None,\n",
    "            'STU, Corn': None,\n",
    "            'Production, US': None,\n",
    "            'Production, AR': None,\n",
    "            'Production, BR': None,\n",
    "        }\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            if row['Commodity'] == \"Soybeans\":\n",
    "                if row['Region'] == 'United States':\n",
    "                    new_row['STU, US'] = row['STU']\n",
    "                    new_row['Production, US'] = row['Production']\n",
    "                elif row['Region'] == 'Argentina':\n",
    "                    new_row['STU, AR'] = row['STU']\n",
    "                    new_row['Production, AR'] = row['Production']\n",
    "                elif row['Region'] == 'Brazil':\n",
    "                    new_row['STU, BR'] = row['STU']\n",
    "                    new_row['Production, BR'] = row['Production']\n",
    "            \n",
    "            if row['Commodity'] == 'Corn' and row['Region'] == 'United States':\n",
    "                new_row['STU, Corn'] = row['STU']\n",
    "        \n",
    "        # Append the new row to the list\n",
    "        rows.append(new_row)\n",
    "\n",
    "    # Create the new DataFrame from the list of new rows\n",
    "    processed_data = pd.DataFrame(rows)\n",
    "    processed_data.to_csv(output_path, index=False)\n",
    "\n",
    "def append_indicators(data_path, indicator_path, cot_path):\n",
    "    # Read the main data and indicators data\n",
    "    df_data = pd.read_csv(data_path)\n",
    "    df_indicator = pd.read_csv(indicator_path)\n",
    "    df_cot = pd.read_csv(cot_path)\n",
    "    \n",
    "    # Convert 'Report Month' in df_data and 'Date' in df_indicator to period format (YYYY-MM)\n",
    "    df_data['Report Month'] = pd.to_datetime(df_data['Report Date']).dt.to_period('M')\n",
    "    df_indicator['Date'] = pd.to_datetime(df_indicator['Date']).dt.to_period('M')\n",
    "    \n",
    "    # Merge the main data with the indicators data on 'Report Month' and 'Date'\n",
    "    merged_df = pd.merge(df_data, df_indicator, how='left', left_on='Report Month', right_on='Date')\n",
    "    \n",
    "    # # Convert 'Report Date' in df_cot to period format (YYYY-MM)\n",
    "    # df_cot['Report Date'] = pd.to_datetime(df_cot['Report_Date']).dt.to_period('M')\n",
    "    \n",
    "    # # Group COT data by 'Report Date' and calculate averages for 'MM Position, Net' and 'MM Position, Gross'\n",
    "    # df_cot['MM Position, Gross'] = df_cot['MM_Long_All'] + df_cot['MM_Short_All']\n",
    "    # df_cot.rename(columns={'Net_Position':'MM Position, Net'}, inplace=True)\n",
    "    # grouped_cot = df_cot.groupby('Report Date').agg({\n",
    "    #     'MM Position, Net': 'mean',\n",
    "    #     'MM Position, Gross': 'mean'\n",
    "    # }).reset_index()\n",
    "    \n",
    "    # # Merge the previously merged DataFrame with the grouped COT data\n",
    "    # final_df = pd.merge(merged_df, grouped_cot, how='left', left_on='Report Date', right_on='Report Date')\n",
    "\n",
    "    merged_df.drop(columns=['Report Month', 'Date'], inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def read_and_process_dates(dates_path):\n",
    "    \"\"\"Reads and processes the complete dates CSV.\"\"\"\n",
    "    data = pd.read_csv(dates_path).iloc[:-1]\n",
    "    data = data.rename(columns={'Time': 'Date'})\n",
    "    data['Date'] = pd.to_datetime(data['Date']).dt.date\n",
    "    return data.sort_values(by='Date', ascending=True)\n",
    "\n",
    "def get_sorted_contracts(csv_files, symbols, years):\n",
    "    \"\"\"Sorts the CSV files by expiration based on symbols and years.\"\"\"\n",
    "    contracts_sorted_by_expiration = []\n",
    "    for year in years:\n",
    "        for symbol in symbols:\n",
    "            contract_name = f\"{symbol}{str(year)[-2:]}.csv\"\n",
    "            if contract_name in csv_files:\n",
    "                contracts_sorted_by_expiration.append(contract_name)\n",
    "    return contracts_sorted_by_expiration\n",
    "\n",
    "def create_tuple(row):\n",
    "    \"\"\"Creates a tuple of High and Low values or returns an empty string if NaN.\"\"\"\n",
    "    return (row['High'], row['Low']) if pd.notna(row['High']) and pd.notna(row['Low']) else ''\n",
    "\n",
    "def merge_contract_data(csv_dir, contracts_sorted_by_expiration, data):\n",
    "    \"\"\"Merges contract data into the main DataFrame.\"\"\"\n",
    "    for contract in contracts_sorted_by_expiration:\n",
    "        contract_name = contract.replace('.csv', '')\n",
    "        path = os.path.join(csv_dir, contract)\n",
    "        \n",
    "        # Read and process the contract CSV\n",
    "        price_df = pd.read_csv(path).iloc[:-1]\n",
    "        price_df = price_df.rename(columns={'Time': 'Date'})\n",
    "        price_df['Date'] = pd.to_datetime(price_df['Date']).dt.date\n",
    "        price_df['Tuple'] = price_df.apply(create_tuple, axis=1)\n",
    "\n",
    "        # Merge into the main DataFrame\n",
    "        data = data.merge(price_df[['Date', 'Tuple']], on='Date', how='left', suffixes=('', f'_{contract_name}'))\n",
    "        data.rename(columns={'Tuple': f'{contract_name}'}, inplace=True)\n",
    "    \n",
    "    return data.fillna('')\n",
    "\n",
    "def unpack_prices(price_string):\n",
    "    \"\"\"Unpacks the High and Low prices from a tuple string.\"\"\"\n",
    "    high, low = map(float, price_string.strip('()').split(','))\n",
    "    return high, low\n",
    "\n",
    "def determine_contract(month, year):\n",
    "    \"\"\"Determines the current and next contract based on the month and year.\"\"\"\n",
    "    yr = str(year)[-2:]  # Get the last two digits of the year\n",
    "    contracts = {\n",
    "        1: (f'SF{yr}', f'SH{yr}'),\n",
    "        2: (f'SH{yr}', f'SH{yr}'),\n",
    "        3: (f'SH{yr}', f'SK{yr}'),\n",
    "        4: (f'SK{yr}', f'SK{yr}'),\n",
    "        5: (f'SK{yr}', f'SN{yr}'),\n",
    "        6: (f'SN{yr}', f'SN{yr}'),\n",
    "        7: (f'SN{yr}', f'SQ{yr}'),\n",
    "        8: (f'SQ{yr}', f'SU{yr}'),\n",
    "        9: (f'SU{yr}', f'SX{yr}'),\n",
    "        10: (f'SX{yr}', f'SX{yr}'),\n",
    "        11: (f'SX{yr}', f'SF{str(int(year) + 1)[-2:]}'),  \n",
    "        12: (f'SF{str(int(year) + 1)[-2:]}', f'SF{str(int(year) + 1)[-2:]}'),\n",
    "    }\n",
    "    return contracts.get(month, ('', ''))\n",
    "\n",
    "def process_continuous_data(dates, wasde_df, historical_df):\n",
    "    \"\"\"Processes continuous futures data by selecting the appropriate contracts.\"\"\"\n",
    "    continuous_data = []\n",
    "    index = 0\n",
    "\n",
    "    for date in dates:\n",
    "        if index < len(wasde_df):\n",
    "            wasde_date = wasde_df.iloc[index]['Report Date'].date()\n",
    "            month, year = wasde_date.month, wasde_date.year\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        current_contract, next_contract = determine_contract(month, year)\n",
    "        contract_name = current_contract if date < wasde_date else next_contract\n",
    "\n",
    "        if date >= wasde_date:\n",
    "            index += 1\n",
    "\n",
    "        if contract_name and not historical_df.loc[historical_df['Date'].dt.date == date, contract_name].empty:\n",
    "            pricing_tuple = historical_df.loc[historical_df['Date'].dt.date == date, contract_name].values[0]\n",
    "            high, low = unpack_prices(pricing_tuple)\n",
    "            average = (high + low) / 2\n",
    "            continuous_data.append([date, contract_name, high, low, average])\n",
    "\n",
    "    return pd.DataFrame(continuous_data, columns=['Date', 'Contract', 'High', 'Low', 'Average'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Aggregate WASDE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "excel_path = '../data/WASDE/World_WASDE_2000-2010.xlsx'\n",
    "csv_paths = ['../data/WASDE/2010-2020/WASDE_2010-2015.csv', '../data/WASDE/2010-2020/WASDE_2016-2020.csv']\n",
    "csv_dir = '../data/WASDE/2021-2024'\n",
    "processed_data_path = '../data/WASDE/processed_WASDE_data.csv'\n",
    "aggregated_data_path = '../data/WASDE/WASDE.csv'\n",
    "\n",
    "# Run the aggregation process\n",
    "process_wasde_data(excel_path, csv_paths, csv_dir, processed_data_path)\n",
    "aggregate_wasde_data(processed_data_path, aggregated_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Pricing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = '../data/soybean_prices/raw'\n",
    "price_dir = '../data/soybean_prices'\n",
    "wasde_path = '../data/WASDE/WASDE.csv'\n",
    "dates_path = os.path.join(price_dir, 'complete_dates.csv')\n",
    "aggregate_price_path = os.path.join(price_dir, 'aggregate_soybeans.csv')\n",
    "continuous_price_path = os.path.join(price_dir, 'continuous_soybeans.csv')\n",
    "\n",
    "# Read and process dates\n",
    "dates_df = read_and_process_dates(dates_path)\n",
    "dates = dates_df['Date'].tolist()\n",
    "\n",
    "# Create the initial DataFrame with dates\n",
    "data = pd.DataFrame(dates, columns=['Date'])\n",
    "\n",
    "# Define the base symbols and the range of years\n",
    "symbols = [\"SF\", \"SH\", \"SK\", \"SN\", \"SQ\", \"SU\", \"SX\"]\n",
    "years = list(range(2000, pd.Timestamp.today().year + 1))\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Sort the CSV files by year and symbol order\n",
    "contracts_sorted_by_expiration = get_sorted_contracts(csv_files, symbols, years)\n",
    "\n",
    "# Merge the contract data\n",
    "data = merge_contract_data(csv_dir, contracts_sorted_by_expiration, data)\n",
    "\n",
    "# Save the final merged DataFrame to a CSV file\n",
    "data.to_csv(aggregate_price_path, index=False)\n",
    "\n",
    "# Read the historical and WASDE DataFrames\n",
    "historical_df = pd.read_csv(aggregate_price_path, parse_dates=['Date'], low_memory=False)\n",
    "wasde_df = pd.read_csv(wasde_path, parse_dates=['Report Date'])\n",
    "\n",
    "# Process continuous futures data\n",
    "continuous_df = process_continuous_data(dates, wasde_df, historical_df)\n",
    "continuous_df.to_csv(continuous_price_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Macroeconomic Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wasde_path = '../data/WASDE/WASDE.csv'\n",
    "soybeans_wasde_path = '../data/WASDE/WASDE_soybeans.csv'\n",
    "macro_path = '../data/macroeconomic_indicators/indicators.csv'\n",
    "cot_path = '../data/commitment_of_traders/soybeans.csv'\n",
    "data_output_path = '../data/soybeans_model_input.csv'\n",
    "\n",
    "process_wasde_soybeans(wasde_path, soybeans_wasde_path)\n",
    "merged_data = append_indicators(soybeans_wasde_path, macro_path, cot_path)\n",
    "merged_data.to_csv(data_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_path = '../data/soybean_prices/continuous_soybeans.csv'\n",
    "data_path = '../data/soybeans_model_input.csv'\n",
    "\n",
    "data_df = pd.read_csv(data_path, low_memory=False)\n",
    "price_df = pd.read_csv(price_path, low_memory=False)\n",
    "\n",
    "data_df.rename(columns={'Report Date':'Date'}, inplace=True)\n",
    "data_df['Date'] = pd.to_datetime(data_df['Date']).dt.date\n",
    "price_df['Date'] = pd.to_datetime(price_df['Date']).dt.date\n",
    "\n",
    "report_dates = data_df['Date'].tolist()\n",
    "report_dates.append(pd.Timestamp.today().date())\n",
    "\n",
    "price_high_list = []\n",
    "price_low_list = []\n",
    "price_average_list = []\n",
    "price_collections = []\n",
    "\n",
    "# Iterate through report dates\n",
    "for i in range(len(report_dates) - 1):\n",
    "    start_date = report_dates[i]\n",
    "    end_date = report_dates[i + 1]\n",
    "\n",
    "    # Filter the price_df for the date range\n",
    "    group = price_df[(price_df['Date'] >= start_date) & (price_df['Date'] < end_date)]\n",
    "    \n",
    "    if not group.empty:\n",
    "        group = group.head(15)  # Limit to the first 15 rows\n",
    "        price_array = group['Average'].to_list()\n",
    "\n",
    "        price_high = group['High'].max()\n",
    "        price_low = group['Low'].min()\n",
    "        price_average = group['Average'].mean()\n",
    "    else:\n",
    "        # If the group is empty, assign NaN or an appropriate default value\n",
    "        price_array = []\n",
    "        price_high = float('nan')\n",
    "        price_low = float('nan')\n",
    "        price_average = float('nan')\n",
    "    \n",
    "    # Append the results to the respective lists\n",
    "    price_high_list.append(float(price_high))\n",
    "    price_low_list.append(float(price_low))\n",
    "    price_average_list.append(float(price_average))\n",
    "    price_collections.append(price_array)\n",
    "\n",
    "# Append the new columns to data_df\n",
    "data_df['Price_High'] = price_high_list\n",
    "data_df['Price_Low'] = price_low_list\n",
    "data_df['Price_Average'] = price_average_list\n",
    "data_df['Average_Price_Collection'] = price_collections\n",
    "\n",
    "data_df.to_csv(data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
